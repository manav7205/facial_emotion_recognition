{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NCi3RbQFonEq",
        "outputId": "c51ea1f3-a6af-489e-80fb-2d5fe7cb72d9",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor Flow Version: 2.11.0\n",
            "Keras Version: 2.11.0\n",
            "\n",
            "Python 3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]\n",
            "Pandas 1.5.3\n",
            "Scikit-Learn 1.2.1\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import tensorflow.keras\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import *\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version: {tensorflow.keras.__version__}\") \n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"Pandas {pd.__version__}\")\n",
        "print(f\"Scikit-Learn {sk.__version__}\")\n",
        "print(tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIdwbFcRonEv",
        "outputId": "e566df25-f11c-4ff2-d031-c81d7998bbe2",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of instances:  35888\n",
            "instance length:  2304\n",
            "28709 train samples\n",
            "3589 test samples\n"
          ]
        }
      ],
      "source": [
        "num_classes = 7\n",
        "img_rows,img_cols = 48,48\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "with open(r\"C:\\Users\\admin\\Desktop\\Major_6700\\fer2013.csv\") as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "lines = np.array(content)\n",
        "\n",
        "num_of_instances = lines.size\n",
        "print(\"number of instances: \",num_of_instances)\n",
        "print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))\n",
        "\n",
        "#------------------------------\n",
        "#initialize trainset and test set\n",
        "x_train, y_train, x_test, y_test = [], [], [], []\n",
        "\n",
        "#------------------------------\n",
        "#transfer train and test set data\n",
        "for i in range(1,num_of_instances):\n",
        "    try:\n",
        "        emotion, img, usage = lines[i].split(\",\")\n",
        "          \n",
        "        val = img.split(\" \")\n",
        "            \n",
        "        pixels = np.array(val, 'float32')\n",
        "        \n",
        "        emotion = to_categorical(emotion, num_classes)\n",
        "    \n",
        "        if 'Training' in usage:\n",
        "            y_train.append(emotion)\n",
        "            x_train.append(pixels)\n",
        "        elif 'PublicTest' in usage:\n",
        "            y_test.append(emotion)\n",
        "            x_test.append(pixels)\n",
        "    except:\n",
        "      print(\"\", end=\"\")\n",
        "\n",
        "#------------------------------\n",
        "#data transformation for train and test sets\n",
        "x_train = np.array(x_train, 'float32')\n",
        "y_train = np.array(y_train, 'float32')\n",
        "x_test = np.array(x_test, 'float32')\n",
        "y_test = np.array(y_test, 'float32')\n",
        "\n",
        "x_train /= 255 #normalize inputs between [0, 1]\n",
        "x_test /= 255\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "91ILRiy9onEx",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_bname_base  = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    F1, F2, F3= filters\n",
        "    \n",
        "    X_shortcut = X\n",
        "    \n",
        "    X = Conv2D(F1, (1,1), (1,1), padding= 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis = -1, name = bn_bname_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(F2, (f, f), (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis = -1, name = bn_bname_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(F3, (1, 1), (1, 1), padding ='valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis = -1, name = bn_bname_base + '2c')(X)\n",
        "    \n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UUeWzmkuonEx",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def convolutional_block(X, f, filters, stage, block, s =2):\n",
        "    \n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    X_shortcut = X\n",
        "    \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis = -1, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(F2, (f,f), strides=(1,1), padding='same', name=conv_name_base+'2b', kernel_initializer = glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis=-1, name = bn_name_base+'2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(F3, (1,1), strides = (1,1), padding = 'valid', name = conv_name_base+'2c', kernel_initializer=glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis=-1, name = bn_name_base + '2c')(X)\n",
        "    \n",
        "    X_shortcut = Conv2D(F3, (1,1), strides=(s,s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform())(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=-1, name=bn_name_base+'1')(X_shortcut)\n",
        "    \n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fy84X9JBonEy",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def ResNet50(input_shape, classes):\n",
        "    x_input = Input(input_shape)\n",
        "    \n",
        "    X = ZeroPadding2D((3,3))(x_input)\n",
        "    \n",
        "    X = Conv2D(64, (3,3), (1,1), name='conv1', kernel_initializer=glorot_uniform())(X)\n",
        "    X = BatchNormalization(axis=-1, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    \n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters = [128,128,512], stage = 3, s=2, block = 'a')\n",
        "    X = identity_block(X, 3, [128,128,512], stage = 3,block ='b')\n",
        "    X = identity_block(X, 3, [128,128,512], stage = 3,block ='c')\n",
        "    X = identity_block(X, 3, [128,128,512], stage = 3,block ='d')\n",
        "\n",
        "    X = convolutional_block(X,  3, [256,256,1024], s=2, stage = 4,block = 'a')\n",
        "    X = identity_block(X, 3, [256,256,1024], stage = 4,block='b')\n",
        "    X = identity_block(X, 3, [256,256,1024], stage = 4,block='c')\n",
        "    X = identity_block(X, 3, [256,256,1024], stage = 4,block='d')\n",
        "    X = identity_block(X, 3, [256,256,1024], stage = 4,block='e')\n",
        "    X = identity_block(X, 3, [256,256,1024], stage = 4,block='f')\n",
        "\n",
        "    \n",
        "    X = convolutional_block(X, 3, [512,512,2048], s = 2,stage = 5,block='a')\n",
        "    X = identity_block(X, 3, [512,512,2048], stage = 5,block = 'b')\n",
        "    X = identity_block(X, 3, [512,512,2048], stage = 5,block = 'c')\n",
        "    \n",
        "    X = AveragePooling2D((2,2), name = 'avg_pool')(X)\n",
        "    \n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name = 'fc' + str(classes), kernel_initializer=glorot_uniform())(X)\n",
        "\n",
        "    model = Model(inputs = x_input, outputs = X, name = 'ResNet50')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yXA9-RQ2onEz",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "model = ResNet50((48,48,1), num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4wrs0tRsonEz",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNKaUMStonE0",
        "outputId": "af65ffc5-e597-400a-dcf6-5d7dbe72abc8",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of training examples = 28709\n",
            "number of test examples = 3589\n",
            "X_train shape: (28709, 48, 48, 1)\n",
            "Y_train shape: (28709, 7)\n",
            "X_test shape: (3589, 48, 48, 1)\n",
            "Y_test shape: (3589, 7)\n"
          ]
        }
      ],
      "source": [
        "print (\"number of training examples = \" + str(x_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(x_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(x_train.shape))\n",
        "print (\"Y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(x_test.shape))\n",
        "print (\"Y_test shape: \" + str(y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UkmHWWSonE0",
        "outputId": "9b168e64-5f6c-418e-ee37-34ce0983c567",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "898/898 [==============================] - 1973s 2s/step - loss: 2.2431 - accuracy: 0.2534\n",
            "Epoch 2/100\n",
            "898/898 [==============================] - 1933s 2s/step - loss: 1.7025 - accuracy: 0.3878\n",
            "Epoch 3/100\n",
            "898/898 [==============================] - 1894s 2s/step - loss: 1.6014 - accuracy: 0.4268\n",
            "Epoch 4/100\n",
            "898/898 [==============================] - 1888s 2s/step - loss: 1.4975 - accuracy: 0.4448\n",
            "Epoch 5/100\n",
            "898/898 [==============================] - 1889s 2s/step - loss: 1.7571 - accuracy: 0.3512\n",
            "Epoch 6/100\n",
            "898/898 [==============================] - 1882s 2s/step - loss: 1.4020 - accuracy: 0.4816\n",
            "Epoch 7/100\n",
            "898/898 [==============================] - 1880s 2s/step - loss: 1.3238 - accuracy: 0.5000\n",
            "Epoch 8/100\n",
            "898/898 [==============================] - 1879s 2s/step - loss: 1.4213 - accuracy: 0.4704\n",
            "Epoch 9/100\n",
            "898/898 [==============================] - 1880s 2s/step - loss: 1.2033 - accuracy: 0.5450\n",
            "Epoch 10/100\n",
            "898/898 [==============================] - 1875s 2s/step - loss: 1.2080 - accuracy: 0.5433\n",
            "Epoch 11/100\n",
            "898/898 [==============================] - 1873s 2s/step - loss: 1.0888 - accuracy: 0.5858\n",
            "Epoch 12/100\n",
            "898/898 [==============================] - 1873s 2s/step - loss: 1.1165 - accuracy: 0.5855\n",
            "Epoch 13/100\n",
            "898/898 [==============================] - 1875s 2s/step - loss: 0.9974 - accuracy: 0.6225\n",
            "Epoch 14/100\n",
            "898/898 [==============================] - 1872s 2s/step - loss: 0.9351 - accuracy: 0.6513\n",
            "Epoch 15/100\n",
            "898/898 [==============================] - 1874s 2s/step - loss: 0.8371 - accuracy: 0.6876\n",
            "Epoch 16/100\n",
            "898/898 [==============================] - 1876s 2s/step - loss: 0.7343 - accuracy: 0.7266\n",
            "Epoch 17/100\n",
            "898/898 [==============================] - 1872s 2s/step - loss: 0.6261 - accuracy: 0.7704\n",
            "Epoch 18/100\n",
            "898/898 [==============================] - 1876s 2s/step - loss: 0.4972 - accuracy: 0.8187\n",
            "Epoch 19/100\n",
            "898/898 [==============================] - 1877s 2s/step - loss: 0.3862 - accuracy: 0.8603\n",
            "Epoch 20/100\n",
            "898/898 [==============================] - 1877s 2s/step - loss: 0.3656 - accuracy: 0.8724\n",
            "Epoch 21/100\n",
            "898/898 [==============================] - 1876s 2s/step - loss: 0.2374 - accuracy: 0.9165\n",
            "Epoch 22/100\n",
            "898/898 [==============================] - 1876s 2s/step - loss: 0.1861 - accuracy: 0.9377\n",
            "Epoch 23/100\n",
            "898/898 [==============================] - 1875s 2s/step - loss: 0.1716 - accuracy: 0.9432\n",
            "Epoch 24/100\n",
            "898/898 [==============================] - 1881s 2s/step - loss: 0.1621 - accuracy: 0.9450\n",
            "Epoch 25/100\n",
            "898/898 [==============================] - 1879s 2s/step - loss: 0.1446 - accuracy: 0.9529\n",
            "Epoch 26/100\n",
            "898/898 [==============================] - 1879s 2s/step - loss: 0.1672 - accuracy: 0.9436\n",
            "Epoch 27/100\n",
            "898/898 [==============================] - 1880s 2s/step - loss: 0.0952 - accuracy: 0.9709\n",
            "Epoch 28/100\n",
            "898/898 [==============================] - 1880s 2s/step - loss: 0.1079 - accuracy: 0.9661\n",
            "Epoch 29/100\n",
            "898/898 [==============================] - 1883s 2s/step - loss: 0.1173 - accuracy: 0.9628\n",
            "Epoch 30/100\n",
            "898/898 [==============================] - 1883s 2s/step - loss: 0.1128 - accuracy: 0.9637\n",
            "Epoch 31/100\n",
            "898/898 [==============================] - 1888s 2s/step - loss: 0.0879 - accuracy: 0.9722\n",
            "Epoch 32/100\n",
            "898/898 [==============================] - 1885s 2s/step - loss: 0.0871 - accuracy: 0.9724\n",
            "Epoch 33/100\n",
            "898/898 [==============================] - 1895s 2s/step - loss: 0.0984 - accuracy: 0.9695\n",
            "Epoch 34/100\n",
            "898/898 [==============================] - 1888s 2s/step - loss: 0.0786 - accuracy: 0.9765\n",
            "Epoch 35/100\n",
            "898/898 [==============================] - 1893s 2s/step - loss: 0.0929 - accuracy: 0.9710\n",
            "Epoch 36/100\n",
            "898/898 [==============================] - 1891s 2s/step - loss: 0.0756 - accuracy: 0.9759\n",
            "Epoch 37/100\n",
            "898/898 [==============================] - 1896s 2s/step - loss: 0.0645 - accuracy: 0.9800\n",
            "Epoch 38/100\n",
            "898/898 [==============================] - 1898s 2s/step - loss: 0.0805 - accuracy: 0.9743\n",
            "Epoch 39/100\n",
            "898/898 [==============================] - 1898s 2s/step - loss: 0.0590 - accuracy: 0.9822\n",
            "Epoch 40/100\n",
            "898/898 [==============================] - 1899s 2s/step - loss: 0.0637 - accuracy: 0.9801\n",
            "Epoch 41/100\n",
            "898/898 [==============================] - 1902s 2s/step - loss: 0.0846 - accuracy: 0.9718\n",
            "Epoch 42/100\n",
            "898/898 [==============================] - 1903s 2s/step - loss: 0.0708 - accuracy: 0.9769\n",
            "Epoch 43/100\n",
            "898/898 [==============================] - 1902s 2s/step - loss: 0.0492 - accuracy: 0.9843\n",
            "Epoch 44/100\n",
            "898/898 [==============================] - 1905s 2s/step - loss: 0.0555 - accuracy: 0.9830\n",
            "Epoch 45/100\n",
            "898/898 [==============================] - 1902s 2s/step - loss: 0.0630 - accuracy: 0.9800\n",
            "Epoch 46/100\n",
            "898/898 [==============================] - 1905s 2s/step - loss: 0.0520 - accuracy: 0.9830\n",
            "Epoch 47/100\n",
            "898/898 [==============================] - 1907s 2s/step - loss: 0.0663 - accuracy: 0.9782\n",
            "Epoch 48/100\n",
            "898/898 [==============================] - 1907s 2s/step - loss: 0.0569 - accuracy: 0.9819\n",
            "Epoch 49/100\n",
            "898/898 [==============================] - 1920s 2s/step - loss: 0.0457 - accuracy: 0.9852\n",
            "Epoch 50/100\n",
            "898/898 [==============================] - 1927s 2s/step - loss: 0.0482 - accuracy: 0.9843\n",
            "Epoch 51/100\n",
            "898/898 [==============================] - 1930s 2s/step - loss: 0.0431 - accuracy: 0.9853\n",
            "Epoch 52/100\n",
            "898/898 [==============================] - 1923s 2s/step - loss: 0.0603 - accuracy: 0.9805\n",
            "Epoch 53/100\n",
            "898/898 [==============================] - 1924s 2s/step - loss: 0.0578 - accuracy: 0.9812\n",
            "Epoch 54/100\n",
            "898/898 [==============================] - 1916s 2s/step - loss: 0.0376 - accuracy: 0.9877\n",
            "Epoch 55/100\n",
            "898/898 [==============================] - 1913s 2s/step - loss: 0.0352 - accuracy: 0.9874\n",
            "Epoch 56/100\n",
            "898/898 [==============================] - 1914s 2s/step - loss: 0.0481 - accuracy: 0.9835\n",
            "Epoch 57/100\n",
            "898/898 [==============================] - 1915s 2s/step - loss: 0.0450 - accuracy: 0.9850\n",
            "Epoch 58/100\n",
            "898/898 [==============================] - 1913s 2s/step - loss: 0.0421 - accuracy: 0.9858\n",
            "Epoch 59/100\n",
            "898/898 [==============================] - 1879s 2s/step - loss: 0.0479 - accuracy: 0.9840\n",
            "Epoch 60/100\n",
            "898/898 [==============================] - 1891s 2s/step - loss: 0.0407 - accuracy: 0.9866\n",
            "Epoch 61/100\n",
            "898/898 [==============================] - 1896s 2s/step - loss: 0.0371 - accuracy: 0.9881\n",
            "Epoch 62/100\n",
            "898/898 [==============================] - 1906s 2s/step - loss: 0.0543 - accuracy: 0.9815\n",
            "Epoch 63/100\n",
            "898/898 [==============================] - 1907s 2s/step - loss: 0.0389 - accuracy: 0.9873\n",
            "Epoch 64/100\n",
            "898/898 [==============================] - 1927s 2s/step - loss: 0.0309 - accuracy: 0.9902\n",
            "Epoch 65/100\n",
            "898/898 [==============================] - 1927s 2s/step - loss: 0.0468 - accuracy: 0.9853\n",
            "Epoch 66/100\n",
            "898/898 [==============================] - 1885s 2s/step - loss: 0.0366 - accuracy: 0.9868\n",
            "Epoch 67/100\n",
            "898/898 [==============================] - 1896s 2s/step - loss: 0.0385 - accuracy: 0.9868\n",
            "Epoch 68/100\n",
            "898/898 [==============================] - 1902s 2s/step - loss: 0.0345 - accuracy: 0.9873\n",
            "Epoch 69/100\n",
            "898/898 [==============================] - 1917s 2s/step - loss: 0.0478 - accuracy: 0.9844\n",
            "Epoch 70/100\n",
            "898/898 [==============================] - 1932s 2s/step - loss: 0.0326 - accuracy: 0.9891\n",
            "Epoch 71/100\n",
            "898/898 [==============================] - 1931s 2s/step - loss: 0.0332 - accuracy: 0.9891\n",
            "Epoch 72/100\n",
            "898/898 [==============================] - 1895s 2s/step - loss: 0.0445 - accuracy: 0.9851\n",
            "Epoch 73/100\n",
            "898/898 [==============================] - 1904s 2s/step - loss: 0.0529 - accuracy: 0.9823\n",
            "Epoch 74/100\n",
            "898/898 [==============================] - 1910s 2s/step - loss: 0.0380 - accuracy: 0.9874\n",
            "Epoch 75/100\n",
            "898/898 [==============================] - 1928s 2s/step - loss: 0.0196 - accuracy: 0.9937\n",
            "Epoch 76/100\n",
            "898/898 [==============================] - 1937s 2s/step - loss: 0.0274 - accuracy: 0.9911\n",
            "Epoch 77/100\n",
            "898/898 [==============================] - 1904s 2s/step - loss: 0.0430 - accuracy: 0.9858\n",
            "Epoch 78/100\n",
            "898/898 [==============================] - 1902s 2s/step - loss: 0.0322 - accuracy: 0.9887\n",
            "Epoch 79/100\n",
            "898/898 [==============================] - 1914s 2s/step - loss: 0.0267 - accuracy: 0.9907\n",
            "Epoch 80/100\n",
            "898/898 [==============================] - 1930s 2s/step - loss: 0.0319 - accuracy: 0.9887\n",
            "Epoch 81/100\n",
            "898/898 [==============================] - 1908s 2s/step - loss: 0.0423 - accuracy: 0.9857\n",
            "Epoch 82/100\n",
            "898/898 [==============================] - 1907s 2s/step - loss: 0.0309 - accuracy: 0.9904\n",
            "Epoch 83/100\n",
            "898/898 [==============================] - 1926s 2s/step - loss: 0.0393 - accuracy: 0.9867\n",
            "Epoch 84/100\n",
            "898/898 [==============================] - 1918s 2s/step - loss: 0.0294 - accuracy: 0.9903\n",
            "Epoch 85/100\n",
            "898/898 [==============================] - 1910s 2s/step - loss: 0.0323 - accuracy: 0.9893\n",
            "Epoch 86/100\n",
            "898/898 [==============================] - 1927s 2s/step - loss: 0.0239 - accuracy: 0.9911\n",
            "Epoch 87/100\n",
            "898/898 [==============================] - 1934s 2s/step - loss: 0.0349 - accuracy: 0.9873\n",
            "Epoch 88/100\n",
            "898/898 [==============================] - 1935s 2s/step - loss: 0.0396 - accuracy: 0.9875\n",
            "Epoch 89/100\n",
            "898/898 [==============================] - 1917s 2s/step - loss: 0.0336 - accuracy: 0.9892\n",
            "Epoch 90/100\n",
            "898/898 [==============================] - 1922s 2s/step - loss: 0.0244 - accuracy: 0.9911\n",
            "Epoch 91/100\n",
            "898/898 [==============================] - 1921s 2s/step - loss: 0.0235 - accuracy: 0.9913\n",
            "Epoch 92/100\n",
            "898/898 [==============================] - 1915s 2s/step - loss: 0.0281 - accuracy: 0.9904\n",
            "Epoch 93/100\n",
            "898/898 [==============================] - 1923s 2s/step - loss: 0.0305 - accuracy: 0.9891\n",
            "Epoch 94/100\n",
            "898/898 [==============================] - 1928s 2s/step - loss: 0.0324 - accuracy: 0.9890\n",
            "Epoch 95/100\n",
            "898/898 [==============================] - 1929s 2s/step - loss: 0.0338 - accuracy: 0.9884\n",
            "Epoch 96/100\n",
            "898/898 [==============================] - 1928s 2s/step - loss: 0.0277 - accuracy: 0.9900\n",
            "Epoch 97/100\n",
            "898/898 [==============================] - 1935s 2s/step - loss: 0.0186 - accuracy: 0.9936\n",
            "Epoch 98/100\n",
            "898/898 [==============================] - 1940s 2s/step - loss: 0.0286 - accuracy: 0.9906\n",
            "Epoch 99/100\n",
            "898/898 [==============================] - 1935s 2s/step - loss: 0.0358 - accuracy: 0.9881\n",
            "Epoch 100/100\n",
            "898/898 [==============================] - 1941s 2s/step - loss: 0.0230 - accuracy: 0.9925\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1e5d214a820>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs = 100, batch_size = 32) #model has been trained for a total of 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ydJcMfdLonE1",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "model=model.save('ResNet50.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cgh99bJ_FeM"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "4cff3abf1678755e0069fd79299a535fe1940bcd71a6b01d9f4386710b2b163f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
